---
layout:     post
title:      pipeline 功能简单剖析
date:       2025-9-24
author:     rky
header-img: img/post-bg-cook.jpg
catalog: true
tags:
    - 标注评测
---

## 开场
### 1.什么是 pipeline ？
Pipeline是我们系统中的**自动化流水线**，用于支撑标注与评测流程。简单来说，Pipeline能够自动完成从数据准备、标注任务创建、人工标注、质量检验（质检还未接入）到评测的整个流程。
- 一句话总结：以事件驱动和 DAG 编排为引擎，以快照和版本机制为保障，并支持多种触发模式和灵活的任务模板配置，实现了可扩展、可追溯的 pipeline 自动化工作流功能，极大提升了任务创建与执行的效率。

举个具体例子，在没有Pipeline之前完成一次标注任务，需要： 
1. 手工创建标注任务，配置30多个标签
2. 手工分配给5个标注员，设置分配比例
3. 等标注完成后，手工创建质检任务
4. 质检完成后，手工导出数据

这整个过程的配置会经常重复，每次要花费半小时左右，而且会有出错的可能。

**有了 Pipeline ，这个流程变成了：**
- 配置一次Pipeline模板（包括数据源、标注模板、通知规则）
- 每一段时间，Pipeline自动触发，完成上述所有步骤
- 整个过程无需人工干预，效率提升巨大

### 1.2 为什么需要 Pipeline 功能？

在我接手这个功能开发之前，我们面临几个严重的痛点：

#### 痛点一：重复性工作太多，人力浪费严重

任务创建是固定的：
- 每两周一次评测
- 每次评测需要标注xxxx条新数据
- 标注任务的配置每次都一样：同样的标签、同样的标注员、同样的分配策略

但在没有自动化系统的情况下，**要手工创建标注任务**。

#### 痛点二：响应速度慢，影响迭代效率

很多时候，数据准备好了，但人还没来，标注任务就无法及时创建。这会导致标注员闲置等待，影响整个迭代周期。

#### 痛点三：出错概率高，问题排查困难

手工操作容易出错：配置错了标签、分配比例算错了、忘记发送通知等等。而且出错后，很难追溯是哪个环节出的问题，排查成本很高。

### 1.3 Pipeline的核心与核心价值


####  核心1：事件驱动的异步架构（最核心）
为什么是核心： 这是整个系统的基石
• Kafka消息队列实现节点间解耦

• 支持长时间运行的任务（标注可能需要几天）

• 节点执行不阻塞，Pipeline可以暂停等待（WAITING状态）

• 通过JobCreatedEvent和StepFinishedEvent驱动流程


技术点： 如果没有事件驱动，Pipeline根本无法支持"创建标注任务后等待几天标注完成"这种场景。

---

####  核心2：DAG编排引擎（最核心）
为什么是核心： 这是Pipeline的智能调度能力
• 支持节点依赖关系（node_002依赖node_001）

• 自动识别就绪节点（依赖都已完成的PENDING节点）

• 支持并行执行（无依赖关系的节点同时执行）

• 通过`findReadyNodes()`算法实现


技术点： 这不是简单的任务串联，而是真正的工作流编排。

---

#### 核心3：快照机制（最核心）
为什么是核心： 保证Pipeline执行的不可变性
• 每次执行创建快照（bm_ml_llm_pipeline_snapshot表）

• 快照记录pipeline_version_id，配置修改不影响执行中的Pipeline

• 支持历史执行的完整追溯

• 是可重现性和可审计性的基础


技术点： 这解决了"执行中修改配置"的一致性问题。

---

#### 核心4：三种触发方式
为什么是核心： 支撑不同的自动化场景
1. 手动触发：临时任务（你说的"定时任务"只是其中之一）
2. 定时触发：周期性任务（cron表达式）
3. 事件触发：数据驱动（离线工作流完成、API写入完成）

技术点： 三种触发方式统一通过`handleDataSetReadyEvent()`执行，这是设计精髓。

---

#### 核心5：四种数据源类型
为什么是核心： 适应不同的数据准备方式
1. 现有数据集：直接使用系统中的数据
2. 文件上传：用户手工上传CSV/Excel
3. API接口：外部系统动态写入数据
4. 离线工作流：通过Kafka感知ETL完成

技术点： 通过策略模式实现，每种类型有独立的处理逻辑。

---

#### 核心6：节点执行器框架
为什么是核心： Pipeline的业务逻辑载体
• DataCollectionNodeExecutor：数据采集（入口节点模式）

• CreateAnnotationNodeExecutor：创建标注任务（这里才用到模板配置）

• WaitForEventNodeExecutor：等待外部事件

• EvaluationNodeExecutor：执行评测


技术点： 使用模板方法模式+工厂模式，易于扩展新节点类型。

---

#### 核心7：版本管理机制
为什么是核心： 支持Pipeline配置的演进
• 通过`is_active`字段标识当前生效版本

• 每次修改创建新版本，旧版本保留

• 快照关联version_id，不受后续修改影响


技术点： 这是可维护性和可追溯性的保障。

---

基于以上痛点，Pipeline功能带来的核心价值是：

**第一、效率提升**：
- 人力与时间节省

**第二、响应更快**：
- 数据就绪后，几分钟内自动创建标注任务
- 支持7×24小时自动化运行

**第三、质量更高**：
- 自动化流程不会因为人工疏忽而配置错误
- 所有操作有完整的日志记录，便于追溯
- 支持多种触发方式(手动、定时)，更加灵活

**第四、可扩展性强**：
- 支持复杂的工作流编排
- 易于添加新的节点类型

---

## 二、核心设计目标与原则

在开始设计Pipeline功能时，我们明确了几个核心目标和设计原则：

### 2.1 功能目标

**目标1：全流程自动化**
- 支持从数据准备到标注完成的全流程自动化
- 无需人工干预，自动推进流程
- 支持长时间运行的复杂流程

**目标2：灵活的触发机制**
- 支持手动触发（临时任务）
- 支持定时触发（周期性任务）
- 支持事件触发（数据驱动）

**目标3：丰富的数据源支持**
- 支持现有数据集（直接使用系统中的数据）
- 支持文件上传（用户手工上传数据）
- 支持API接口（外部系统动态写入）
- 支持离线工作流（通过Kafka消息感知）

**目标4：完善的监控和通知**
- 实时查看Pipeline执行状态
- 关键节点自动发送通知
- 支持详细的执行日志查询

### 2.2 技术设计原则

**原则1：事件驱动的异步架构**

这是整个Pipeline系统的核心设计理念。我们没有采用传统的同步调用方式，而是使用事件驱动的异步架构。

**为什么选择事件驱动？**

最开始，我们考虑过同步调用的方案：
```java
// 同步调用方案（我们放弃的方案）
dataCollectionNode.execute();
createAnnotationNode.execute();
evaluationNode.execute();
```

这种方案有几个严重的问题：
1. **无法支持长时间运行的节点**：比如标注任务可能需要几天才能完成，同步调用会一直阻塞
2. **无法支持并行执行**：如果多个节点可以并行执行，同步调用无法利用这个优势
3. **容错能力差**：某个节点失败后，整个流程就失败了，无法重试
4. **耦合度高**：节点之间紧密耦合，难以扩展

所以我们选择了**事件驱动的异步架构**：
```java
// 事件驱动方案（我们的选择）
节点执行完成 → 发布StepFinishedEvent → Kafka队列 → EventHandler消费 → 推进下一个节点
```

这种方案的好处是：
- **解耦**：节点执行和流程编排完全解耦，互不影响
- **异步**：节点可以长时间运行，不阻塞主流程
- **可重试**：节点执行失败可以重试，不影响其他节点
- **可扩展**：容易添加新的节点类型和事件类型
- **可监控**：可以监控事件流转，快速发现问题

**原则2：DAG（有向无环图）编排**

Pipeline的节点关系是DAG结构，支持复杂的依赖关系和并行执行。

**为什么选择DAG而不是简单的链式结构？**

最开始，我们考虑过简单的链式结构：
```
节点A → 节点B → 节点C → 节点D
```

但实际业务场景往往更复杂：
- 节点B和节点C可能都依赖节点A，但它们之间没有依赖关系，可以并行执行
- 节点D可能同时依赖节点B和节点C，需要等两个节点都完成后才能执行

所以我们选择了DAG结构：
```
        节点A
       /    \
    节点B   节点C
       \    /
        节点D
```

DAG编排的核心逻辑是：
1. 分析DAG结构，找出所有的依赖关系
2. 找出可以执行的节点（依赖节点都已完成的PENDING节点）
3. 并行启动所有就绪的节点
4. 等待节点完成事件，继续推进流程

**原则3：快照机制保证不可变性**

每次Pipeline触发，都会创建一个Snapshot（快照），记录本次执行的完整配置信息。

**为什么需要快照机制？**

如下是一个可能遇到的问题，比如：

**问题场景：**
- 1月1日，用户配置了一个Pipeline，数据源是数据集A
- 1月2日，Pipeline自动触发，开始执行
- 1月3日，用户修改了Pipeline配置，数据源改为数据集B
- 1月4日，Pipeline再次触发，应该使用哪个配置？

**我们的解决方案是快照机制：**
- 每次Pipeline触发时，创建一个不可变的快照
- 快照记录了本次执行的完整配置：Pipeline版本、数据源配置、模板配置、通知配置
- 即使用户修改了Pipeline配置，已经在执行中的Pipeline仍然使用原来的快照配置
- 下次触发时，使用新的配置创建新的快照

这样设计的好处是：
- **不可变性**：执行中的Pipeline配置不会被修改影响
- **可追溯性**：可以查看任何历史执行的完整配置
- **可对比性**：可以对比不同版本的Pipeline配置差异

**原则4：策略模式支持扩展**

Pipeline需要支持多种数据源类型和节点类型，我们使用策略模式来实现。
**我们的策略模式设计：**
- **每个策略独立**：每种数据源的逻辑完全独立，职责清晰
- **易于扩展**：添加新数据源只需实现新的策略类，不需要修改主流程
- **易于测试**：每个策略可以独立测试
- **Spring自动发现**：新的策略类会被Spring自动扫描并注册
---

## 三、整体技术方案设计

### 3.1 系统架构设计

Pipeline系统采用分层架构，包含触发管理层、编排服务层、事件发布层、节点执行层、通知服务层、数据存储层六个核心层次。

```
┌─────────────────────────────────────────────────────────┐
│                   触发管理层                               │
│  ManualTriggerHandler | ScheduledTriggerHandler |       │
│  DatasetReadyTriggerHandler                             │
└─────────────────────────────────────────────────────────┘
                          ↓
┌─────────────────────────────────────────────────────────┐
│                   编排服务层                               │
│  PipelineOrchestratorService | DatasetIdResolver        │
└─────────────────────────────────────────────────────────┘
                          ↓
┌─────────────────────────────────────────────────────────┐
│                   事件发布层                               │
│  PipelineEventPublisher → Kafka → EventHandler          │
└─────────────────────────────────────────────────────────┘
                          ↓
┌─────────────────────────────────────────────────────────┐
│                   节点执行层                               │
│  DataCollectionNodeExecutor | CreateAnnotationExecutor  │
│  EvaluationExecutor | WaitForEventExecutor              │
└─────────────────────────────────────────────────────────┘
                          ↓
┌─────────────────────────────────────────────────────────┐
│                   通知服务层                               │
│  NotificationService | DaxiangService                   │
└─────────────────────────────────────────────────────────┘
                          ↓
┌─────────────────────────────────────────────────────────┐
│                   数据存储层                               │
│  MySQL | HBase | Kafka                                  │
└─────────────────────────────────────────────────────────┘
```

接下来详细介绍每一层的设计：

#### 3.1.1 触发管理层

触发管理层负责处理Pipeline的三种触发方式，这是Pipeline执行的入口。

**核心组件：**

**1. PipelineTriggerManager（触发管理器）**

这是触发层的统一入口，负责根据触发类型分发到具体的处理器：

**2. ManualTriggerHandler（手动触发处理器）**

处理用户在界面上点击"立即运行"的场景：

**3. ScheduledTriggerHandler（定时触发处理器）**
Pipeline的定时触发需求非常灵活：
- 用户可以随时修改cron表达式
- 需要支持多种周期：每天、每周、每两周、自定义间隔
- 需要支持有效期控制：超过有效期自动停止触发

**4. DatasetReadyTriggerHandler（数据就绪触发处理器）**

这是最复杂的触发处理器，用于处理数据驱动的触发场景。

**典型场景：**
- 离线工作流执行完成后，通过Kafka消息通知Pipeline
- 外部系统通过API接口写入数据后，触发Pipeline
- 用户上传文件完成后，触发Pipeline

#### 3.1.2 编排服务层

编排服务层是Pipeline系统的大脑，负责DAG编排、节点调度、状态管理等核心逻辑。

**核心组件：**

**1. PipelineOrchestratorService（Pipeline编排服务）**

这是整个系统最核心的服务类

**核心方法一：handleDataSetReadyEvent（处理数据集就绪事件）**

这是Pipeline执行的真正入口：

细解释每一步的设计考虑：

**步骤1：查找Pipeline配置**

这里有一个重要的设计：我们通过`is_active`字段来标识当前生效的Pipeline版本。

**为什么需要版本管理？**

Pipeline的配置可能会经常修改：
- 修改数据源配置
- 修改标注模板
- 修改通知规则

如果直接修改原有记录，会导致：
- 正在执行中的Pipeline受影响（配置被修改了）
- 无法追溯历史配置（不知道之前的配置是什么）

所以我们设计了版本管理机制：
- 每次修改配置，创建新的version记录
- 新version的`is_active=1`，旧version的`is_active=0`
- 查询时只查询`is_active=1`的版本
- 执行中的Pipeline使用snapshot中记录的version_id，不受新版本影响

**步骤2：解析DAG定义**

DAG定义存储在`dag_definition`字段中，是一个JSON格式的字符串：

**步骤3：创建Pipeline执行快照**
快照记录包含：
- `pipeline_id`：Pipeline主键
- `pipeline_version_id`：**关键字段**，记录了使用的版本
- `dataset_id`：数据集ID
- `status`：执行状态（RUNNING/COMPLETED/FAILED）
- `context`：执行上下文（JSON格式）
- `start_time`：开始时间
- `end_time`：结束时间

**步骤4：创建节点快照**

为每个节点创建快照记录，初始状态为PENDING：


节点快照记录包含：
- `pipeline_snapshot_id`：关联Pipeline快照
- `node_id`：节点ID
- `node_type`：节点类型
- `status`：节点状态（PENDING/RUNNING/COMPLETED/FAILED/WAITING）
- `input_dataset_id`：输入数据集ID
- `output_dataset_id`：输出数据集ID
- `error_message`：错误信息（失败时记录）
- `start_time`：开始时间
- `end_time`：结束时间

**步骤5：启动起始节点**

找出所有没有依赖的节点（起始节点），并行启动它们：

**这里有一个关键的设计决策：为什么用Kafka消息而不是直接调用节点执行器？**

我当时考虑过两种方案：

**方案一：直接调用节点执行器**
```java
for (PipelineNodeBO node : initialNodes) {
    NodeExecutor executor = nodeExecutorFactory.getExecutor(node.getNodeType());
    executor.execute(node, context);  // 同步调用
}
```

**方案二：通过Kafka消息异步调用**
```java
for (PipelineNodeBO node : initialNodes) {
    publishJobCreatedEvent(...);  // 发布消息到Kafka
}
```

我最终选择了**方案二**，理由是：
1. **解耦**：编排服务和节点执行完全解耦
2. **异步**：不会阻塞编排服务的主流程
3. **可扩展**：节点执行可以部署在不同的机器上
4. **可重试**：Kafka消息有重试机制
5. **可监控**：可以监控Kafka消息的消费情况

**核心方法二：handleStepFinishedEvent（处理步骤完成事件）**

这是Pipeline推进的核心逻辑，当节点执行完成后，通过这个方法来推进Pipeline：


**核心方法三：progressPipeline（推进Pipeline执行）**

这是Pipeline编排的核心算法，负责找出下一个可以执行的节点：


**核心算法：findReadyNodes（找出就绪节点）**

这是DAG编排的核心算法：

这个算法的核心思想是：
1. 遍历所有节点
2. 找出状态为PENDING的节点
3. 检查这个节点的所有依赖节点是否都已完成
4. 如果都已完成，这个节点就是就绪节点，可以启动执行

这个算法保证了：
- 节点按照依赖关系顺序执行
- 没有依赖关系的节点可以并行执行
- 不会出现死锁（因为是DAG，没有环）

**2. DatasetIdResolver（数据集ID解析器）**

这个类负责根据数据源类型解析真实的数据集ID。

**为什么需要这个类？**

因为不同的数据源类型，获取数据集ID的方式不同：
- **现有数据集**：直接使用配置的datasetId
- **上传文件**：通过上传接口获取datasetId
- **API接口**：通过API调用写入后获取datasetId
- **离线工作流**：通过Kafka消息获取datasetId


